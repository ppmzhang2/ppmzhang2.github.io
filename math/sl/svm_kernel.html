<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>SVM: Kernel Method &#8212; Machine Learning Notes</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=d75fae25" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic_mod.css?v=9b2032db" />
    <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "displayMath": [["$$", "$$"], ["\\[", "\\]"]]}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/disqus.js?v=568ff333"></script>
    <script src="../../_static/js/theme.js"></script>
    <script src="../../_static/js/petite-vue.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Gradient in Backpropagation" href="bp.html" />
    <link rel="prev" title="Linear Regression: Ridge Regression" href="lr_ridge.html" /> 
  </head><body data-dark_mode_code_blocks="true">

<div id="top_nav">
    

    <nav>
        
            
        

        <p id="toggle_sidebar">
            <a href="#" title="Toggle sidebar">|||</a>
        </p>
        <h1><a href="../../index.html" title="Go to homepage">ML Notes</a></h1>
            <a id="source_link" href="https://github.com/ppmzhang2/ppmzhang2.github.io">
    
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512">
            <path fill="white" d="M 244.8,8 C 106.1,8 0,113.3 0,252 c 0,110.9 69.8,205.8 169.5,239.2 12.8,2.3 17.3,-5.6 17.3,-12.1 0,-6.2 -0.3,-40.4 -0.3,-61.4 0,0 -70,15 -84.7,-29.8 0,0 -11.4,-29.1 -27.8,-36.6 0,0 -22.9,-15.7 1.6,-15.4 0,0 24.9,2 38.6,25.8 21.9,38.6 58.6,27.5 72.9,20.9 2.3,-16 8.8,-27.1 16,-33.7 -55.9,-6.2 -112.3,-14.3 -112.3,-110.5 0,-27.5 7.6,-41.3 23.6,-58.9 -2.6,-6.5 -11.1,-33.3 2.6,-67.9 20.9,-6.5 69,27 69,27 20,-5.6 41.5,-8.5 62.8,-8.5 21.3,0 42.8,2.9 62.8,8.5 0,0 48.1,-33.6 69,-27 13.7,34.7 5.2,61.4 2.6,67.9 16,17.7 25.8,31.5 25.8,58.9 0,96.5 -58.9,104.2 -114.8,110.5 9.2,7.9 17,22.9 17,46.4 0,33.7 -0.3,75.4 -0.3,83.6 0,6.5 4.6,14.4 17.3,12.1 C 428.2,457.8 496,362.9 496,252 496,113.3 383.5,8 244.8,8 Z"/>
        </svg>
    
</a>
        

        <a id="mode_toggle" href="#" @click.prevent="handleClick" :title="mode">
    <template v-if="mode == 'light'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_light"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M67.48,18.073c1.913,-1.912 1.913,-5.018 0,-6.931c-1.912,-1.912 -5.018,-1.912 -6.931,0l-6.798,6.799c-1.912,1.912 -1.912,5.018 0,6.931c1.913,1.912 5.018,1.912 6.931,-0l6.798,-6.799Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.728,61.108c1.912,-1.913 1.912,-5.018 -0,-6.931c-1.913,-1.913 -5.019,-1.913 -6.931,-0l-6.799,6.798c-1.912,1.913 -1.912,5.019 0,6.931c1.913,1.913 5.019,1.913 6.931,0l6.799,-6.798Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.682,54.177c-1.913,-1.913 -5.018,-1.913 -6.931,-0c-1.912,1.913 -1.912,5.018 0,6.931l6.798,6.798c1.913,1.913 5.019,1.913 6.931,0c1.913,-1.912 1.913,-5.018 0,-6.931l-6.798,-6.798Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M4.901,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,-0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M18.929,11.142c-1.912,-1.912 -5.018,-1.912 -6.931,0c-1.912,1.913 -1.912,5.019 0,6.931l6.799,6.799c1.912,1.912 5.018,1.912 6.931,-0c1.912,-1.913 1.912,-5.019 -0,-6.931l-6.799,-6.799Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.108,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c-0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c-0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'dark'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_dark"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.901,2.196 -4.901,4.901c0,2.705 2.197,4.901 4.901,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.662,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.989,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.732,61.103c1.91,-1.91 1.91,-5.011 0,-6.921l-0.009,-0.01c-1.91,-1.91 -5.012,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.909,1.91 5.011,1.91 6.92,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.672,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.52,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l-0,-0.01c-0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.901,2.196 -4.901,4.901c0,2.704 2.197,4.9 4.901,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.73,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 -0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.098,34.623c-2.699,0 -4.891,2.192 -4.891,4.892l-0,0.019c-0,2.699 2.192,4.891 4.891,4.891c2.7,0 4.892,-2.192 4.892,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.892,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'darkest'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_darkest"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><path d="M39.315,23.791c8.684,-0 15.734,7.05 15.734,15.733c0,8.684 -7.05,15.734 -15.734,15.734c-8.683,0 -15.733,-7.05 -15.733,-15.734c-0,-8.683 7.05,-15.733 15.733,-15.733Zm0,4.737c6.069,0 10.997,4.927 10.997,10.996c-0,6.069 -4.928,10.996 -10.997,10.996c-6.068,0 -10.996,-4.927 -10.996,-10.996c0,-6.069 4.928,-10.996 10.996,-10.996Z" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.216,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.9,2.196 -4.9,4.901c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.666,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.99,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.737,61.103c1.909,-1.91 1.909,-5.011 -0,-6.921l-0.01,-0.01c-1.91,-1.91 -5.011,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.91,1.91 5.011,1.91 6.921,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.676,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.524,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l0,-0.01c0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.216,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901c-0,2.704 2.196,4.9 4.9,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.734,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.103,34.623c-2.7,0 -4.892,2.192 -4.892,4.892l-0,0.019c-0,2.699 2.192,4.891 4.892,4.891c2.699,0 4.891,-2.192 4.891,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.891,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>
</a>

<script>
(function() {
    const LOCAL_STORAGE_KEY = 'piccoloThemeMode'

    var initialMode = localStorage.getItem(LOCAL_STORAGE_KEY)

    if (initialMode) {
        // Make sure the value in local storage is valid
        if (['light', 'dark', 'darkest'].indexOf(initialMode) == -1) {
            initialMode = 'light'
            localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
        }
    } else {
        // Check if the client prefers dark mode
        if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
            initialMode = 'dark'
        } else {
            initialMode = 'light'
        }
        localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
    }

    document.documentElement.dataset.mode = initialMode

    PetiteVue.createApp({
        'mode': initialMode,
        handleClick() {
            let currentMode = this.mode

            if (currentMode == 'light') {
                this.mode = 'dark'
            } else if (currentMode == 'dark') {
                this.mode = 'darkest'
            } else if (currentMode == 'darkest') {
                this.mode = 'light'
            }

            document.documentElement.dataset.mode = this.mode
            localStorage.setItem(LOCAL_STORAGE_KEY, this.mode)

            console.log(this.mode)
        }
    }).mount('#mode_toggle')
})()
</script>
            <p class="mobile_search_link">
                <a href="../../search.html" title="Search">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 65 64" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2">
                        <path d="M14.873 40.009c-2.315-3.943-3.642-8.532-3.642-13.429C11.231 11.91 23.141 0 37.811 0s26.58 11.91 26.58 26.58-11.91 26.58-26.58 26.58a26.44 26.44 0 0 1-14.277-4.161L9.739 62.794a3.12 3.12 0 0 1-4.413 0L.913 58.382c-1.217-1.218-1.217-3.196 0-4.413l13.96-13.96zM37.811 8.054c10.225 0 18.526 8.301 18.526 18.526s-8.301 18.526-18.526 18.526-18.526-8.301-18.526-18.526S27.586 8.054 37.811 8.054z" fill="#fff" />
                    </svg>
                </a>
            </p>
        

        <div class="searchbox_wrapper">
            
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
    </nav>
</div>

    
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper"><p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Mathematics</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../ft/index.html">Fourier Transform</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Statistical Learning</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mse.html">Mean Squared Error Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayes.html">Bayes Theorem</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayes_ex_1.html">Bayes Example: Biased Coin</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayes_ex_2.html">Bayes Example: Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="mgd.html">Multivariate Gaussian Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="lda.html">LDA: Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="lda_dr.html">LDA: Dimensionality Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="qda.html">Quadratic Discriminant Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="lr_formula.html">Linear Regression: Formula Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lr_snr.html">Linear Regression: Signal-to-Noise Ratio (SNR)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lr_ols.html">Linear Regression: Ordinary Least Squares</a></li>
<li class="toctree-l3"><a class="reference internal" href="lr_ols_formula.html">Linear Regression: OLS Formula</a></li>
<li class="toctree-l3"><a class="reference internal" href="lr_ridge.html">Linear Regression: Ridge Regression</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">SVM: Kernel Method</a></li>
<li class="toctree-l3"><a class="reference internal" href="bp.html">Gradient in Backpropagation</a></li>
<li class="toctree-l3"><a class="reference internal" href="gd.html">Gradient Descent</a></li>
<li class="toctree-l3"><a class="reference internal" href="pg.html">Policy Gradient</a></li>
<li class="toctree-l3"><a class="reference internal" href="appx_a.html">Appendix A: Setup R Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="appx_b.html">Appendix B: Setup Jupyter Lab Environment</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../misc/index.html">Miscellaneous</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../nlp/index.html">NLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cv/index.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mnp/index.html">Micro-nanoplastics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ref.html">Reference</a></li>
</ul>

        </div>
      </div>


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="svm-kernel-method">
<h1>SVM: Kernel Method<a class="headerlink" href="#svm-kernel-method" title="Link to this heading">¶</a></h1>
<p>We seek <span class="math notranslate nohighlight">\(f \in \mathcal{H}\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is a
<strong>Reproducing Kernel Hilbert Space</strong> (<strong>RKHS</strong>) of real-valued functions on
<span class="math notranslate nohighlight">\(\mathcal{X} \subseteq \mathbb{R}^d\)</span>.</p>
<div class="math notranslate nohighlight">
\[
f : \mathcal{X} \to \mathbb{R}, \quad
\mathcal{X} \subseteq \mathbb{R}^d
\]</div>
<p>By the definition of RKHS, the pointwise evaluation in <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> for each
<span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span></p>
<div class="math notranslate nohighlight">
\[
L_x : f \mapsto f(x)
\]</div>
<p>is a <strong>continuous linear functional</strong> for all <span class="math notranslate nohighlight">\(f \in \mathcal{H}\)</span>.</p>
<p>By the <strong>Riesz Representation Theorem</strong>:</p>
<div class="math notranslate nohighlight">
\[
\forall x \in \mathcal{X}: \quad
\exists k_x \in \mathcal{H}
\quad S.T. \quad
f(x) = L_x (f) = \langle f, k_x \rangle_{\mathcal{H}} \quad
\forall f \in \mathcal{H}
\]</div>
<p>To confirm the <strong>reproducing property</strong>, take <span class="math notranslate nohighlight">\(f = k_x\)</span>:</p>
<div class="math notranslate nohighlight">
\[
k_x (y) = L_y (k_x) = \langle k_x, k_y \rangle_{\mathcal{H}}
\]</div>
<p>We define the <strong>reproducing kernel</strong> as a function
<span class="math notranslate nohighlight">\(k : \mathcal{X} \times \mathcal{X} \to \mathbb{R}\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[
k(x, y) = \langle k_x, k_y \rangle_{\mathcal{H}}
\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the RKHS setting, the concept of “dimensionality increase” is abstract:
functions live in an infinite-dimensional space.
However, the “effective dimension” is governed by the kernel <span class="math notranslate nohighlight">\(k\)</span>, which defines
the structure of <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>.
Once a kernel is chosen, the hypothesis space is fixed, and no additional
dimensions are introduced during learning.
Later the representer theorem also shows that the solution is a finite linear
combination of kernel functions evaluated at the training points.
In fact, some people use the notation <span class="math notranslate nohighlight">\(\mathcal{H}_k\)</span> to denote the RKHS
associated with kernel <span class="math notranslate nohighlight">\(k\)</span>.
For simplicity we use <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>.</p>
</div>
<section id="empirical-risk">
<h2>Empirical Risk<a class="headerlink" href="#empirical-risk" title="Link to this heading">¶</a></h2>
<p>To find the optimal function <span class="math notranslate nohighlight">\(f^*\)</span>, we need to minimize the empirical risk:</p>
<div class="math notranslate nohighlight">
\[
f^* =
\min_{f \in \mathcal{H}} \; R(\|f\|_{\mathcal{H}}^2) + E(f(x_1), \ldots, f(x_n))
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((x_i, y_i) \in \mathcal{X} \times \mathbb{R}\)</span> are the training data,</p></li>
<li><p><span class="math notranslate nohighlight">\(R\)</span> is a non-decreasing regularizer (e.g., <span class="math notranslate nohighlight">\(R(u) = \lambda u\)</span>),</p></li>
<li><p><span class="math notranslate nohighlight">\(E\)</span> is a empirical loss function (e.g., squared loss, hinge loss).</p></li>
</ul>
<p>By <strong>representer theorem</strong>, any minimizer of the above problem admits the form:</p>
<div class="math notranslate nohighlight">
\[
f^* = \sum_{i=1}^n \alpha_i k_{x_i}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_i \in \mathbb{R}\)</span>.</p>
<p>We will use it to compute the empirical risk.
First, we compute the norm:</p>
<div class="math notranslate nohighlight">
\[
\|f\|_{\mathcal{H}}^2 =
\left\langle
  \sum_{i=1}^n \alpha_i k_{x_i}, \sum_{j=1}^n \alpha_j k_{x_j}
\right\rangle_{\mathcal{H}} =
\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j
\langle k_{x_i}, k_{x_j} \rangle_{\mathcal{H}} =
\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j k(x_i, x_j)
\]</div>
<div class="math notranslate nohighlight">
\[
\therefore
\|f\|_{\mathcal{H}}^2 = \alpha^\top K \alpha
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(K \in \mathbb{R}^{n \times n}\)</span> is the <strong>Gram matrix</strong> with entries <span class="math notranslate nohighlight">\(K_{ij} = k(x_i, x_j)\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}^n\)</span> is the vector of coefficients <span class="math notranslate nohighlight">\(\alpha_i\)</span>.</p></li>
</ul>
<p>The empirical risk depends on the predictions on the training data, which is a vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{f} =
\begin{bmatrix}
f(x_1) \\
f(x_2) \\
\vdots \\
f(x_n)
\end{bmatrix} =
\begin{bmatrix}
\sum_{i=1}^n \alpha_i k(x_1, x_i) \\
\sum_{i=1}^n \alpha_i k(x_2, x_i) \\
\vdots \\
\sum_{i=1}^n \alpha_i k(x_n, x_i)
\end{bmatrix}
\end{split}\]</div>
<p>Expanded as a matrix-vector product:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{f} =
\begin{bmatrix}
k(x_1, x_1) &amp; \cdots &amp; k(x_1, x_n) \\
k(x_2, x_1) &amp; \cdots &amp; k(x_2, x_n) \\
\vdots &amp; \ddots &amp; \vdots \\
k(x_n, x_1) &amp; \cdots &amp; k(x_n, x_n)
\end{bmatrix}
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
\vdots \\
\alpha_n
\end{bmatrix}
\end{split}\]</div>
<p>Which is precisely the definition of a matrix-vector product:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f} = K \alpha
\]</div>
</section>
<section id="derivation-of-solution">
<h2>Derivation of Solution<a class="headerlink" href="#derivation-of-solution" title="Link to this heading">¶</a></h2>
<p>Now the empirical risk can be expressed as
(for simplicity, we will use squared loss):</p>
<div class="math notranslate nohighlight">
\[
\min_{\alpha \in \mathbb{R}^n} \; \frac{\lambda}{2} \alpha^\top K \alpha + \frac{1}{2} \|K \alpha - y\|^2
\]</div>
<p>To minimize, take the gradient w.r.t. <span class="math notranslate nohighlight">\(\alpha\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\nabla_\alpha \left( \frac{\lambda}{2} \alpha^\top K \alpha + \frac{1}{2} \|K \alpha - y\|^2 \right)
= \lambda K \alpha + K (K \alpha - y)
= K (\lambda \alpha + K \alpha - y)
\]</div>
<p>Set gradient to zero:</p>
<div class="math notranslate nohighlight">
\[
K (\lambda \alpha + K \alpha - y) = 0
\]</div>
<p>Assuming <span class="math notranslate nohighlight">\(K\)</span> is positive definite (or at least invertible), this implies:</p>
<div class="math notranslate nohighlight">
\[
(\lambda I + K) \alpha = y
\]</div>
<p><strong>Solution</strong>:</p>
<div class="math notranslate nohighlight">
\[
\boxed{
\alpha = (\lambda I + K)^{-1} y
}
\]</div>
</section>
<section id="appendix-proof-of-riesz-representation-theorem">
<h2>Appendix: Proof of Riesz Representation Theorem<a class="headerlink" href="#appendix-proof-of-riesz-representation-theorem" title="Link to this heading">¶</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> be a Hilbert space of real-valued functions on
<span class="math notranslate nohighlight">\(\mathcal{X} \subseteq \mathbb{R}^d\)</span>, equipped with an inner product
<span class="math notranslate nohighlight">\(\langle \cdot, \cdot \rangle_{\mathcal{H}}\)</span>.
For any <em>bounded linear functional</em> <span class="math notranslate nohighlight">\(L: \mathcal{H} \to \mathbb{R}\)</span>,
we want to show that
there <em>exists a unique</em> <span class="math notranslate nohighlight">\(g \in \mathcal{H}\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\forall f \in \mathcal{H}: \quad
L(f) = \langle f, g \rangle_{\mathcal{H}}
\]</div>
<p><strong>Proof</strong>:</p>
<p>For any <span class="math notranslate nohighlight">\(f \in \mathcal{H}\)</span>, for the trivial case when <span class="math notranslate nohighlight">\(L(f) = 0\)</span>,
we can always take <span class="math notranslate nohighlight">\(g = 0\)</span>, which gives <span class="math notranslate nohighlight">\(\langle f, 0 \rangle = 0 = L(f)\)</span>.</p>
<p>Let us consider the case when <span class="math notranslate nohighlight">\(L(f) \ne 0\)</span> for some <span class="math notranslate nohighlight">\(f \in \mathcal{H}\)</span>.</p>
<p>First, we define the <strong>kernel</strong> and the <strong>orthogonal complement</strong> of the kernel of <span class="math notranslate nohighlight">\(L\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\ker L = \{ f \in \mathcal{H} : L(f) = 0 \}.
\]</div>
<div class="math notranslate nohighlight">
\[
(\ker L)^\perp =
\{ g \in \mathcal{H} : \langle g, f \rangle = 0, \quad \forall f \in \ker L \}
\]</div>
<p>Since <span class="math notranslate nohighlight">\(L\)</span> is a bounded linear operator, <span class="math notranslate nohighlight">\(\ker(L)\)</span> is a <em>closed subspace</em> of
<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. Thus we can apply the <strong>orthogonal decomposition theorem</strong>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{H} = \ker(L) \oplus (\ker L)^\perp
\]</div>
<p>and any <span class="math notranslate nohighlight">\(f \in \mathcal{H}\)</span> can be uniquely decomposed as:</p>
<div class="math notranslate nohighlight">
\[
f = f_0 + g_0
\]</div>
<p>where <span class="math notranslate nohighlight">\(f_0 \in \ker(L)\)</span> and <span class="math notranslate nohighlight">\(g_0 \in (\ker L)^\perp\)</span>.
Note that <span class="math notranslate nohighlight">\(g_0 \ne 0\)</span> as <span class="math notranslate nohighlight">\(L(f) \ne 0\)</span>.</p>
<p>Let:</p>
<div class="math notranslate nohighlight">
\[
g = \frac{L(g_0)}{\|g_0\|^2} g_0
\]</div>
<div class="math notranslate nohighlight">
\[
\therefore
\langle g_0, g \rangle =
\left\langle g_0, \frac{L(g_0)}{\|g_0\|^2} g_0 \right\rangle =
\frac{\|g_0\|^2}{\|g_0\|^2} L(g_0) = L(g_0).
\]</div>
<div class="math notranslate nohighlight">
\[
\because
L(f) = L(f_0 + g_0) = 0 + L(g_0) = L(g_0).
\]</div>
<div class="math notranslate nohighlight">
\[
\because
\langle f, g \rangle =
\langle f_0 + g_0, g \rangle = \langle g_0, g \rangle =
L(g_0)
\]</div>
<div class="math notranslate nohighlight">
\[
\therefore
L(f) = \langle f, g \rangle_{\mathcal{H}}
\]</div>
<p>To prove the uniqueness of <span class="math notranslate nohighlight">\(g\)</span>, suppose there are <span class="math notranslate nohighlight">\(g_1, g_2 \in \mathcal{H}\)</span>
such that</p>
<div class="math notranslate nohighlight">
\[
L(f) = \langle f, g_1 \rangle = \langle f, g_2 \rangle
\quad \forall f \in \mathcal{H}.
\]</div>
<div class="math notranslate nohighlight">
\[
\therefore
\langle f, g_1 - g_2 \rangle = 0
\quad \forall f \in \mathcal{H}.
\]</div>
<p>Take <span class="math notranslate nohighlight">\(f = g_1 - g_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\|g_1 - g_2\|^2 = 0 \Rightarrow g_1 = g_2.
\]</div>
<div class="math notranslate nohighlight">
\[
\tag*{$\blacksquare$}
\]</div>
<p>Back to <a class="reference internal" href="index.html"><span class="doc">Statistical Learning</span></a>.</p>
<div id="disqus_thread" data-identifier="math/sl/svm_kernel"></div></section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
    
        <div id="show_right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">&lt;</span><span>Page contents</span></a></p>
        </div>

        <div id="right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">&gt;</span><span>Page contents:</span></a></p>
            <div class="page_toc">
                <ul>
<li><a class="reference internal" href="#">SVM: Kernel Method</a><ul>
<li><a class="reference internal" href="#empirical-risk">Empirical Risk</a></li>
<li><a class="reference internal" href="#derivation-of-solution">Derivation of Solution</a></li>
<li><a class="reference internal" href="#appendix-proof-of-riesz-representation-theorem">Appendix: Proof of Riesz Representation Theorem</a></li>
</ul>
</li>
</ul>

            </div>
        </div>
    

      <div class="clearer"></div>
    </div>
    <div class="button_nav_wrapper">
        <div class="button_nav">
            <div class="left">
                
                <a href="lr_ridge.html">
                    <span class="icon">&lt;</span><span>Linear Regression: Ridge Regression</span></a>
                
            </div>

            <div class="right">
                
                    <a href="bp.html"><span>Gradient in Backpropagation</span><span class="icon">&gt;</span></a>
                
            </div>
        </div>
    </div>


    <div class="footer" role="contentinfo">
    &#169; Copyright 2022, Juan Cervantes.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.2.3.
    </div>

<p id="theme_credit">Styled using the <a href="https://github.com/piccolo-orm/piccolo_theme">Piccolo Theme</a></p>
  </body>
</html>