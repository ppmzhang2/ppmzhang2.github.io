<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Linear Regression: Ordinary Least Squares &#8212; Machine Learning Notes</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=d75fae25" />
    <link rel="stylesheet" type="text/css" href="../../_static/basic_mod.css?v=9b2032db" />
    <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "displayMath": [["$$", "$$"], ["\\[", "\\]"]]}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/disqus.js?v=568ff333"></script>
    <script src="../../_static/js/theme.js"></script>
    <script src="../../_static/js/petite-vue.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Linear Regression: OLS Formula" href="lr_ols_formula.html" />
    <link rel="prev" title="Linear Regression: Signal-to-Noise Ratio (SNR)" href="lr_snr.html" /> 
  </head><body data-dark_mode_code_blocks="true">

<div id="top_nav">
    

    <nav>
        
            
        

        <p id="toggle_sidebar">
            <a href="#" title="Toggle sidebar">|||</a>
        </p>
        <h1><a href="../../index.html" title="Go to homepage">ML Notes</a></h1>
            <a id="source_link" href="https://github.com/ppmzhang2/ppmzhang2.github.io">
    
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512">
            <path fill="white" d="M 244.8,8 C 106.1,8 0,113.3 0,252 c 0,110.9 69.8,205.8 169.5,239.2 12.8,2.3 17.3,-5.6 17.3,-12.1 0,-6.2 -0.3,-40.4 -0.3,-61.4 0,0 -70,15 -84.7,-29.8 0,0 -11.4,-29.1 -27.8,-36.6 0,0 -22.9,-15.7 1.6,-15.4 0,0 24.9,2 38.6,25.8 21.9,38.6 58.6,27.5 72.9,20.9 2.3,-16 8.8,-27.1 16,-33.7 -55.9,-6.2 -112.3,-14.3 -112.3,-110.5 0,-27.5 7.6,-41.3 23.6,-58.9 -2.6,-6.5 -11.1,-33.3 2.6,-67.9 20.9,-6.5 69,27 69,27 20,-5.6 41.5,-8.5 62.8,-8.5 21.3,0 42.8,2.9 62.8,8.5 0,0 48.1,-33.6 69,-27 13.7,34.7 5.2,61.4 2.6,67.9 16,17.7 25.8,31.5 25.8,58.9 0,96.5 -58.9,104.2 -114.8,110.5 9.2,7.9 17,22.9 17,46.4 0,33.7 -0.3,75.4 -0.3,83.6 0,6.5 4.6,14.4 17.3,12.1 C 428.2,457.8 496,362.9 496,252 496,113.3 383.5,8 244.8,8 Z"/>
        </svg>
    
</a>
        

        <a id="mode_toggle" href="#" @click.prevent="handleClick" :title="mode">
    <template v-if="mode == 'light'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_light"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M67.48,18.073c1.913,-1.912 1.913,-5.018 0,-6.931c-1.912,-1.912 -5.018,-1.912 -6.931,0l-6.798,6.799c-1.912,1.912 -1.912,5.018 0,6.931c1.913,1.912 5.018,1.912 6.931,-0l6.798,-6.799Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.728,61.108c1.912,-1.913 1.912,-5.018 -0,-6.931c-1.913,-1.913 -5.019,-1.913 -6.931,-0l-6.799,6.798c-1.912,1.913 -1.912,5.019 0,6.931c1.913,1.913 5.019,1.913 6.931,0l6.799,-6.798Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.682,54.177c-1.913,-1.913 -5.018,-1.913 -6.931,-0c-1.912,1.913 -1.912,5.018 0,6.931l6.798,6.798c1.913,1.913 5.019,1.913 6.931,0c1.913,-1.912 1.913,-5.018 0,-6.931l-6.798,-6.798Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M4.901,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901l-0,9.614c-0,2.705 2.196,4.901 4.9,4.901c2.705,-0 4.901,-2.196 4.901,-4.901l0,-9.614Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M18.929,11.142c-1.912,-1.912 -5.018,-1.912 -6.931,0c-1.912,1.913 -1.912,5.019 0,6.931l6.799,6.799c1.912,1.912 5.018,1.912 6.931,-0c1.912,-1.913 1.912,-5.019 -0,-6.931l-6.799,-6.799Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.108,34.623c-2.705,0 -4.901,2.196 -4.901,4.901c-0,2.705 2.196,4.901 4.901,4.901l9.614,0c2.705,0 4.901,-2.196 4.901,-4.901c-0,-2.705 -2.196,-4.901 -4.901,-4.901l-9.614,0Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'dark'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_dark"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><circle cx="39.311" cy="39.524" r="15.734" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.212,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.901,2.196 -4.901,4.901c0,2.705 2.197,4.901 4.901,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.662,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.989,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.732,61.103c1.91,-1.91 1.91,-5.011 0,-6.921l-0.009,-0.01c-1.91,-1.91 -5.012,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.909,1.91 5.011,1.91 6.92,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.672,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.52,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l-0,-0.01c-0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.212,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.901,2.196 -4.901,4.901c0,2.704 2.197,4.9 4.901,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.73,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 -0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.098,34.623c-2.699,0 -4.891,2.192 -4.891,4.892l-0,0.019c-0,2.699 2.192,4.891 4.891,4.891c2.7,0 4.892,-2.192 4.892,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.892,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>

    <template v-if="mode == 'darkest'">
        <svg width="100%" height="100%" viewBox="0 0 79 80" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;"><g id="mode_darkest"><rect id="Bounds" x="0" y="-0" width="78.623" height="79.049" style="fill:none;"/><path d="M39.315,23.791c8.684,-0 15.734,7.05 15.734,15.733c0,8.684 -7.05,15.734 -15.734,15.734c-8.683,0 -15.733,-7.05 -15.733,-15.734c-0,-8.683 7.05,-15.733 15.733,-15.733Zm0,4.737c6.069,0 10.997,4.927 10.997,10.996c-0,6.069 -4.928,10.996 -10.997,10.996c-6.068,0 -10.996,-4.927 -10.996,-10.996c0,-6.069 4.928,-10.996 10.996,-10.996Z" style="fill:#fff;"/><g id="beams"><g id="beam"><path id="beam1" serif:id="beam" d="M44.216,14.515c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,0 -4.9,2.196 -4.9,4.901c-0,2.705 2.196,4.901 4.9,4.901c2.705,0 4.901,-2.196 4.901,-4.901Z" style="fill:#fff;"/></g><g id="beam2" serif:id="beam"><path id="beam3" serif:id="beam" d="M60.666,24.892c1.902,-1.902 1.902,-4.99 0,-6.892l-0.04,-0.039c-1.901,-1.902 -4.989,-1.902 -6.891,-0c-1.901,1.901 -1.901,4.989 0,6.891l0.04,0.04c1.902,1.901 4.99,1.901 6.891,-0Z" style="fill:#fff;"/></g><g id="beam4" serif:id="beam"><path id="beam5" serif:id="beam" d="M25.737,61.103c1.909,-1.91 1.909,-5.011 -0,-6.921l-0.01,-0.01c-1.91,-1.91 -5.011,-1.91 -6.921,-0c-1.91,1.91 -1.91,5.011 -0,6.921l0.01,0.01c1.91,1.91 5.011,1.91 6.921,-0Z" style="fill:#fff;"/></g><g id="beam6" serif:id="beam"><path id="beam7" serif:id="beam" d="M60.676,54.167c-1.907,-1.907 -5.004,-1.907 -6.911,0l-0.02,0.02c-1.907,1.907 -1.907,5.004 0,6.911c1.907,1.907 5.004,1.907 6.911,-0l0.02,-0.02c1.907,-1.907 1.907,-5.004 0,-6.911Z" style="fill:#fff;"/></g><g id="beam8" serif:id="beam"><path id="beam9" serif:id="beam" d="M14.524,34.623c-2.702,0 -4.896,2.194 -4.896,4.896l0,0.01c0,2.702 2.194,4.896 4.896,4.896c2.702,0 4.896,-2.194 4.896,-4.896l0,-0.01c0,-2.702 -2.194,-4.896 -4.896,-4.896Z" style="fill:#fff;"/></g><g id="beam10" serif:id="beam"><path id="beam11" serif:id="beam" d="M44.216,64.534c0,-2.705 -2.196,-4.901 -4.901,-4.901c-2.704,-0 -4.9,2.196 -4.9,4.901c-0,2.704 2.196,4.9 4.9,4.9c2.705,0 4.901,-2.196 4.901,-4.9Z" style="fill:#fff;"/></g><g id="beam12" serif:id="beam"><path id="beam13" serif:id="beam" d="M25.734,17.943c-1.911,-1.911 -5.015,-1.911 -6.926,0l-0.005,0.005c-1.911,1.911 -1.911,5.015 0,6.926c1.911,1.911 5.015,1.911 6.926,0l0.005,-0.005c1.911,-1.911 1.911,-5.014 0,-6.926Z" style="fill:#fff;"/></g><g id="beam14" serif:id="beam"><path id="beam15" serif:id="beam" d="M64.103,34.623c-2.7,0 -4.892,2.192 -4.892,4.892l-0,0.019c-0,2.699 2.192,4.891 4.892,4.891c2.699,0 4.891,-2.192 4.891,-4.891l0,-0.019c0,-2.7 -2.192,-4.892 -4.891,-4.892Z" style="fill:#fff;"/></g></g></g></svg>
    </template>
</a>

<script>
(function() {
    const LOCAL_STORAGE_KEY = 'piccoloThemeMode'

    var initialMode = localStorage.getItem(LOCAL_STORAGE_KEY)

    if (initialMode) {
        // Make sure the value in local storage is valid
        if (['light', 'dark', 'darkest'].indexOf(initialMode) == -1) {
            initialMode = 'light'
            localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
        }
    } else {
        // Check if the client prefers dark mode
        if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
            initialMode = 'dark'
        } else {
            initialMode = 'light'
        }
        localStorage.setItem(LOCAL_STORAGE_KEY, initialMode)
    }

    document.documentElement.dataset.mode = initialMode

    PetiteVue.createApp({
        'mode': initialMode,
        handleClick() {
            let currentMode = this.mode

            if (currentMode == 'light') {
                this.mode = 'dark'
            } else if (currentMode == 'dark') {
                this.mode = 'darkest'
            } else if (currentMode == 'darkest') {
                this.mode = 'light'
            }

            document.documentElement.dataset.mode = this.mode
            localStorage.setItem(LOCAL_STORAGE_KEY, this.mode)

            console.log(this.mode)
        }
    }).mount('#mode_toggle')
})()
</script>
            <p class="mobile_search_link">
                <a href="../../search.html" title="Search">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 65 64" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2">
                        <path d="M14.873 40.009c-2.315-3.943-3.642-8.532-3.642-13.429C11.231 11.91 23.141 0 37.811 0s26.58 11.91 26.58 26.58-11.91 26.58-26.58 26.58a26.44 26.44 0 0 1-14.277-4.161L9.739 62.794a3.12 3.12 0 0 1-4.413 0L.913 58.382c-1.217-1.218-1.217-3.196 0-4.413l13.96-13.96zM37.811 8.054c10.225 0 18.526 8.301 18.526 18.526s-8.301 18.526-18.526 18.526-18.526-8.301-18.526-18.526S27.586 8.054 37.811 8.054z" fill="#fff" />
                    </svg>
                </a>
            </p>
        

        <div class="searchbox_wrapper">
            
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
    </nav>
</div>

    
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper"><p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Mathematics</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../ft/index.html">Fourier Transform</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Statistical Learning</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mse.html">Mean Squared Error Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayes.html">Bayes Theorem</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayes_ex_1.html">Bayes Example: Biased Coin</a></li>
<li class="toctree-l3"><a class="reference internal" href="bayes_ex_2.html">Bayes Example: Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="mgd.html">Multivariate Gaussian Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="lda.html">LDA: Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="lda_dr.html">LDA: Dimensionality Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="qda.html">Quadratic Discriminant Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="lr_formula.html">Linear Regression: Formula Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lr_snr.html">Linear Regression: Signal-to-Noise Ratio (SNR)</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Linear Regression: Ordinary Least Squares</a></li>
<li class="toctree-l3"><a class="reference internal" href="lr_ols_formula.html">Linear Regression: OLS Formula</a></li>
<li class="toctree-l3"><a class="reference internal" href="lr_ridge.html">Linear Regression: Ridge Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="svm_kernel.html">SVM: Kernel Method</a></li>
<li class="toctree-l3"><a class="reference internal" href="bp.html">Gradient in Backpropagation</a></li>
<li class="toctree-l3"><a class="reference internal" href="gd.html">Gradient Descent</a></li>
<li class="toctree-l3"><a class="reference internal" href="pg.html">Policy Gradient</a></li>
<li class="toctree-l3"><a class="reference internal" href="appx_a.html">Appendix A: Setup R Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="appx_b.html">Appendix B: Setup Jupyter Lab Environment</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../misc/index.html">Miscellaneous</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../nlp/index.html">NLP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cv/index.html">Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mnp/index.html">Micro-nanoplastics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ref.html">Reference</a></li>
</ul>

        </div>
      </div>


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression-ordinary-least-squares">
<h1>Linear Regression: Ordinary Least Squares<a class="headerlink" href="#linear-regression-ordinary-least-squares" title="Link to this heading">¶</a></h1>
<section id="tl-dr">
<h2>TL;DR<a class="headerlink" href="#tl-dr" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>OLS estimator is unbiased if:</p>
<ul>
<li><p>No Collinearity</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{E} \left[ \epsilon_i \right] = 0,\)</span>
<span class="math notranslate nohighlight">\(\forall i \in \left[ 1, n \right]\)</span></p></li>
</ul>
</li>
<li><p>OLS estimator is the Best Linear Unbiased Estimator (BLUE) if:</p>
<ul>
<li><p>No Collinearity</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{E} \left[ \epsilon_i \right] = 0,\)</span>
<span class="math notranslate nohighlight">\(\forall i \in \left[ 1, n \right]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{Var} \left[ \epsilon_i \right] = \sigma^2,\)</span>
<span class="math notranslate nohighlight">\(\forall i \in \left[ 1, n \right]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{Cov} \left[ \epsilon_i, \epsilon_j \right] = 0,\)</span>
<span class="math notranslate nohighlight">\(\forall i \ne j\)</span></p></li>
</ul>
</li>
<li><p>OLS estimator is equivalent to MLE (generalized linear model) if:</p>
<ul>
<li><p>No Collinearity</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{E} \left[ \epsilon_i \right] = 0,\)</span>
<span class="math notranslate nohighlight">\(\forall i \in \left[ 1, n \right]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{Var} \left[ \epsilon_i \right] = \sigma^2,\)</span>
<span class="math notranslate nohighlight">\(\forall i \in \left[ 1, n \right]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{Cov} \left[ \epsilon_i, \epsilon_j \right] = 0,\)</span>
<span class="math notranslate nohighlight">\(\forall i \ne j\)</span></p></li>
<li><p>random errors are <strong>identically</strong> and <strong>independently</strong> drawn from a
<strong>normal distribution</strong>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="hypothesis">
<h2>Hypothesis<a class="headerlink" href="#hypothesis" title="Link to this heading">¶</a></h2>
<p>By <a class="reference internal" href="lr_formula.html#ref-sl-lr-functional"><span class="std std-ref">previous derivation</span></a> we conclude that the linear
predictor <span class="math notranslate nohighlight">\(f : \mathbb{R}^p \mapsto \mathbb{R}\)</span> is a linear functional:</p>
<div class="math notranslate nohighlight">
\[
f (x) = x^\top \beta
\]</div>
<p>where <span class="math notranslate nohighlight">\(x \in \mathbb{R}^p\)</span> as the input vector, <span class="math notranslate nohighlight">\(\beta \in \mathbb{R}^p\)</span> as the
linear coefficients.</p>
<p>We suppose that there is a <strong>underlying non-random unobservable coefficients</strong>
<span class="math notranslate nohighlight">\(\beta^{\ast} \in \mathbb{R}^p\)</span> such that for every data point
<span class="math notranslate nohighlight">\(\{ x, y \} \in \mathbb{R}^p \times \mathbb{R}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
y = x^\top \beta^{\ast} + \varepsilon
\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon\)</span> is a random error term, which is assumed to be independent
of the input vector <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Also from <a class="reference internal" href="lr_formula.html#ref-sl-formula-ls"><span class="std std-ref">previous chapter</span></a>, the Ordinary Least Squares
(OLS) solution can be expressed as:</p>
<div class="math notranslate nohighlight">
\[
\beta_{\text{ols}} = (X^\top X)^{-1} X^\top y
\]</div>
<p>If <strong>no collinearity</strong> holds i.e. <span class="math notranslate nohighlight">\(X^\top X\)</span> is invertible.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\therefore
\beta_{\text{ols}}
&amp;= (X^\top X)^{-1} X^\top (X \beta^{\ast} + \epsilon) \\
&amp;= (X^\top X)^{-1} X^\top X \beta^{\ast} +
   (X^\top X)^{-1} X^\top \epsilon \\
&amp;= \beta^{\ast} + (X^\top X)^{-1} X^\top \epsilon
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is the vector of random errors
<span class="math notranslate nohighlight">\(\varepsilon_1, \ldots, \varepsilon_n\)</span>.</p>
</section>
<section id="zero-mean-error-and-unbiased-estimator">
<h2>Zero Mean Error and Unbiased Estimator<a class="headerlink" href="#zero-mean-error-and-unbiased-estimator" title="Link to this heading">¶</a></h2>
<div class="math notranslate nohighlight">
\[
\mathrm{E} \left[ \epsilon \right] = 0
\implies
\mathrm{E} \left[ \beta_{\text{ols}} \right] = \beta^{\ast}
\]</div>
<p>Proof:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathrm{E} \left[
  \beta_{\text{ols}}
\right]
&amp;= \mathrm{E} \left[
  \beta^{\ast} + (X^\top X)^{-1} X^\top \epsilon
\right] \\
&amp;= \beta^{\ast} + X^\top (X^\top X)^{-1} \mathrm{E} \left[ \epsilon \right] \\
&amp;= \beta^{\ast}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\tag*{$\blacksquare$}\]</div>
</section>
<section id="homoscedasticity-and-blue">
<h2>Homoscedasticity and BLUE<a class="headerlink" href="#homoscedasticity-and-blue" title="Link to this heading">¶</a></h2>
<p>The OLS estimator is the Best Linear Unbiased Estimator (BLUE) if:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\forall i \in \left[ 0, n \right] &amp;:
\mathrm{E} \left[ \epsilon \right] = 0,
\mathrm{Var} \left[ \epsilon \right] = \sigma^2 \\
\forall i \ne j &amp;:
\mathrm{Cov} \left[ \epsilon \right] = 0
\end{split}\]</div>
<p>Proof<span id="id1">[<a class="reference internal" href="../../ref.html#id14" title="Andrew Rothman. Ols regression, gauss-markov, blue, and understanding the math. Jun 2020. [Online; accessed 21-February-2023]. URL: https://towardsdatascience.com/ ols-linear-regression-gauss-markov-blue-and-understanding-the- math-453d7cc630a5.">17</a>]</span> <span id="id2">[<a class="reference internal" href="../../ref.html#id8" title="Wikipedia. Gauss–Markov Theorem. 2023. [Online; accessed 21-February-2023]. URL: https://en.wikipedia.org/wiki/Gauss–Markov_theorem.">19</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\epsilon \epsilon^T &amp;=
\begin{bmatrix}
  \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n
\end{bmatrix}
\cdot
\begin{bmatrix}
  \varepsilon_1 &amp; \varepsilon_2 &amp; \cdots &amp; \varepsilon_n
\end{bmatrix} \\
&amp;= \begin{bmatrix}
  \varepsilon_1 \varepsilon_1 &amp; \varepsilon_1 \varepsilon_2 &amp; \cdots &amp;
    \varepsilon_1 \varepsilon_n \\
  \varepsilon_2 \varepsilon_1 &amp; \varepsilon_2 \varepsilon_2 &amp; \cdots &amp;
    \varepsilon_2 \varepsilon_n \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \varepsilon_n \varepsilon_1 &amp; \varepsilon_n \varepsilon_2 &amp; \cdots &amp;
    \varepsilon_n \varepsilon_n
\end{bmatrix} \\
&amp;= \begin{bmatrix}
  (\varepsilon_1 - 0) (\varepsilon_1 - 0) &amp;
  (\varepsilon_1 - 0) (\varepsilon_2 - 0) &amp; \cdots &amp;
  (\varepsilon_1 - 0) (\varepsilon_n - 0) \\
  (\varepsilon_2 - 0) (\varepsilon_1 - 0) &amp;
  (\varepsilon_2 - 0) (\varepsilon_2 - 0) &amp; \cdots &amp;
  (\varepsilon_2 - 0) (\varepsilon_n - 0) \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  (\varepsilon_n - 0) (\varepsilon_1 - 0) &amp;
  (\varepsilon_n - 0) (\varepsilon_2 - 0) &amp; \cdots &amp;
  (\epsilon_n - 0) (\epsilon_n - 0)
\end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\therefore
\mathrm{E} \left[
  \epsilon \epsilon^T
\right] &amp;=
\begin{bmatrix}
  \mathrm{E} \left[
    (\varepsilon_1 - 0) (\varepsilon_1 - 0)
  \right] &amp;
  \mathrm{E} \left[
    (\varepsilon_1 - 0) (\varepsilon_2 - 0)
  \right] &amp;
  \cdots &amp;
  \mathrm{E} \left[
    (\varepsilon_1 - 0) (\varepsilon_n - 0)
  \right] \\
  \mathrm{E} \left[
    (\varepsilon_2 - 0) (\varepsilon_1 - 0)
  \right] &amp;
  \mathrm{E} \left[
    (\varepsilon_2 - 0) (\varepsilon_2 - 0)
  \right] &amp;
  \cdots &amp;
  \mathrm{E} \left[
    (\varepsilon_2 - 0) (\varepsilon_n - 0)
  \right] \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \mathrm{E} \left[
    (\varepsilon_n - 0) (\varepsilon_1 - 0)
  \right] &amp;
  \mathrm{E} \left[
    (\varepsilon_n - 0) (\varepsilon_2 - 0)
  \right] &amp;
  \cdots &amp;
  \mathrm{E} \left[
    (\varepsilon_n - 0) (\varepsilon_n - 0)
  \right]
\end{bmatrix}
\\ &amp;=
\begin{bmatrix}
  \mathrm{Var} \left[ \varepsilon_1 \right] &amp;
  \mathrm{Cov} \left[ \varepsilon_1, \varepsilon_2 \right] &amp;
  \cdots &amp;
  \mathrm{Cov} \left[ \varepsilon_1, \varepsilon_n \right] \\
  \mathrm{Cov} \left[ \varepsilon_2, \varepsilon_1 \right] &amp;
  \mathrm{Var} \left[ \varepsilon_2 \right] &amp;
  \cdots &amp;
  \mathrm{Cov} \left[ \varepsilon_2, \varepsilon_n \right] \\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
  \mathrm{Cov} \left[ \varepsilon_n, \varepsilon_1 \right] &amp;
  \mathrm{Cov} \left[ \varepsilon_n, \varepsilon_2 \right] &amp;
  \cdots &amp;
  \mathrm{Var} \left[ \varepsilon_n \right]
\end{bmatrix} \\
&amp;= \sigma^2 I
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\therefore
\mathrm{Var} \left[ \beta_{\text{ols}} \right]
&amp;= \mathrm{E} \left[
  (\beta_{\text{ols}} - \mathrm{E} \left[ \beta_{\text{ols}} \right])
  (\beta_{\text{ols}} - \mathrm{E} \left[ \beta_{\text{ols}} \right])^T
\right] \\
&amp;= \mathrm{E} \left[
  ((X^\top X)^{-1} X^\top \epsilon)
  ((X^\top X)^{-1} X^\top \epsilon)^T
\right] \\
&amp;= \mathrm{E} \left[
  (X^\top X)^{-1} X^\top \epsilon
  \epsilon^\top X (X^\top X)^{-1}
\right] \\
&amp;= (X^\top X)^{-1} X^\top
  \mathrm{E} \left[
    \epsilon \epsilon^\top
  \right]
  X (X^\top X)^{-1} \\
&amp;= \sigma^2 (X^\top X)^{-1}
  X^\top X (X^\top X)^{-1} \\
&amp;= \sigma^2 (X^\top X)^{-1}
\end{split}\]</div>
<p>Let <span class="math notranslate nohighlight">\(\tilde{\beta} = \tilde{C} y\)</span> be another
<strong>unbiased linear estimator</strong> of <span class="math notranslate nohighlight">\(\beta^\ast\)</span> with
<span class="math notranslate nohighlight">\(\tilde{C} = (X^\top X)^{-1} X^\top + D\)</span>
where <span class="math notranslate nohighlight">\(D\)</span> is a <span class="math notranslate nohighlight">\(p \times n\)</span> non-zero matrix.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\therefore
\mathrm{E} \left[ \tilde{\beta} \right]
&amp;= \mathrm{E} \left[ \tilde{C} y \right] \\
&amp;= \mathrm{E} \left[
  ((X^\top X)^{-1} X^\top + D)
  (X \beta^\ast + \epsilon)
\right] \\
&amp;= ((X^\top X)^{-1} X^\top + D) X \beta^\ast +
   ((X^\top X)^{-1} X^\top + D)
   \mathrm{E} \left[ \epsilon \right] \\
&amp;= ((X^\top X)^{-1} X^\top + D) X \beta^\ast \\
&amp;= \beta^\ast + D X \beta^\ast
\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\tilde{\beta}\)</span> is unbiased:</p>
<div class="math notranslate nohighlight">
\[
\therefore
D X = 0
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\therefore
\mathrm{Var} \left[ \tilde{\beta} \right]
&amp;= \mathrm{Var} \left[ \tilde{C} y \right] \\
&amp;= \tilde{C} \mathrm{Var} \left[ y \right] \tilde{C}^\top \\
&amp;= \sigma^2 \tilde{C} \tilde{C}^T \\
&amp;= \sigma^2 ((X^\top X)^{-1} X^\top + D) ((X^\top X)^{-1} X^\top + D)^T \\
&amp;= \sigma^2 ((X^\top X)^{-1} X^\top + D) (X (X^\top X)^{-1} + D^T) \\
&amp;= \sigma^2 ((X^\top X)^{-1} + (X^\top X)^{-1} X^\top D^\top +
              D X (X^\top X)^{-1} + D D^\top) \\
&amp;= \sigma^2 (X^\top X)^{-1} + \sigma^2 (X^\top X)^{-1} (D X)^\top +
   \sigma^2 D X (X^\top X)^{-1} + \sigma^2 D D^\top
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
\because
D X = 0
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\therefore
\mathrm{Var} \left[ \tilde{\beta} \right]
&amp;= \sigma^2 (X^\top X)^{-1} + \sigma^2 D D^T \\
&amp;= \mathrm{Var} \left[ \beta_{\text{ols}} \right] + \sigma^2 D D^\top
\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(D D^\top\)</span> is positive semidefinite matrix:</p>
<div class="math notranslate nohighlight">
\[
\therefore
\mathrm{Var} \left[ \tilde{\beta} \right] &gt;
\mathrm{Var} \left[ \beta_{\text{ols}} \right]
\]</div>
<div class="math notranslate nohighlight">
\[\tag*{$\blacksquare$}\]</div>
</section>
<section id="normally-distributed-error-and-mle">
<h2>Normally Distributed Error and MLE<a class="headerlink" href="#normally-distributed-error-and-mle" title="Link to this heading">¶</a></h2>
<p>The OLS is mathematically equivalent to Maximum Likelihood Estimation if
the error term <span class="math notranslate nohighlight">\(\varepsilon_1, \ldots, \varepsilon_n\)</span> are identically and
independently distributed from a normal distribution of zero mean.</p>
<p>Proof:</p>
<div class="math notranslate nohighlight">
\[
\because
\varepsilon_i =
y_i - \hat{y}_i =
y_i - x_i \beta
\sim
N(\mu, 0)
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\therefore
\mathcal{L} (\beta \mid X) &amp;=
\prod_{i=1}^{n} \frac{1}{\sigma \sqrt{2 \pi}}
e^{-\frac{(y_i - x_i \beta)^2}{2 \sigma^2}}
\\ &amp;=
(\frac{1}{\sigma \sqrt{2 \pi}})^n
\prod_{i=1}^{n}
e^{-\frac{(y_i - x_i \beta)^2}{2 \sigma^2}}
\\ &amp;=
(2 \pi \sigma^2)^{-\frac{n}{2}}
\prod_{i=1}^{n}
e^{-\frac{(y_i - x_i \beta)^2}{2 \sigma^2}}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\therefore
\ln \mathcal{L} (\beta \mid X)
&amp;= -\frac{n}{2} \ln (2 \pi \sigma^2) +
   \sum_{i=1}^n -\frac{(y_i - x_i \beta)^2}{2 \sigma^2} \\
&amp;= -\frac{n}{2} \ln (2 \pi \sigma^2) -
   \frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - x_i \beta)^2 \\
&amp;= -\frac{n}{2} \ln (2 \pi \sigma^2) -
   \frac{1}{2 \sigma^2} (y - X \beta)^\top (y - X \beta)
\end{split}\]</div>
<p>To minimize <span class="math notranslate nohighlight">\(\ln \mathcal{L}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla_{\beta} \ln \mathcal{L} (\beta \mid X) = 0
\\
\implies
(y - X \beta)^T
(y - X \beta) = 0
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
\therefore
\beta_{\text{mle}}
= (X^\top X)^{-1} X^\top y
= \beta_{\text{ols}}
\]</div>
<div class="math notranslate nohighlight">
\[\tag*{$\blacksquare$}\]</div>
<hr class="docutils" />
<p>Back to <a class="reference internal" href="index.html"><span class="doc">Statistical Learning</span></a>.</p>
<div id="disqus_thread" data-identifier="math/sl/lr_ols"></div></section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
    
        <div id="show_right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">&lt;</span><span>Page contents</span></a></p>
        </div>

        <div id="right_sidebar">
            <p><a class="toggle_right_sidebar" href="#"><span class="icon">&gt;</span><span>Page contents:</span></a></p>
            <div class="page_toc">
                <ul>
<li><a class="reference internal" href="#">Linear Regression: Ordinary Least Squares</a><ul>
<li><a class="reference internal" href="#tl-dr">TL;DR</a></li>
<li><a class="reference internal" href="#hypothesis">Hypothesis</a></li>
<li><a class="reference internal" href="#zero-mean-error-and-unbiased-estimator">Zero Mean Error and Unbiased Estimator</a></li>
<li><a class="reference internal" href="#homoscedasticity-and-blue">Homoscedasticity and BLUE</a></li>
<li><a class="reference internal" href="#normally-distributed-error-and-mle">Normally Distributed Error and MLE</a></li>
</ul>
</li>
</ul>

            </div>
        </div>
    

      <div class="clearer"></div>
    </div>
    <div class="button_nav_wrapper">
        <div class="button_nav">
            <div class="left">
                
                <a href="lr_snr.html">
                    <span class="icon">&lt;</span><span>Linear Regression: Signal-to-Noise Ratio (SNR)</span></a>
                
            </div>

            <div class="right">
                
                    <a href="lr_ols_formula.html"><span>Linear Regression: OLS Formula</span><span class="icon">&gt;</span></a>
                
            </div>
        </div>
    </div>


    <div class="footer" role="contentinfo">
    &#169; Copyright 2022, Juan Cervantes.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.2.3.
    </div>

<p id="theme_credit">Styled using the <a href="https://github.com/piccolo-orm/piccolo_theme">Piccolo Theme</a></p>
  </body>
</html>